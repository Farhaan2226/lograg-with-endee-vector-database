LogRAG is a local, secure Retrieval-Augmented Generation (RAG) system that helps engineers understand, diagnose, and resolve production issues by analyzing historical logs and generating structured explanations using a local Large Language Model (LLM).

The system is designed to remain partially functional even when the LLM backend is unavailable, making it resilient and production-aware.

ğŸš€ Key Features

ğŸ” Semantic Log Search using vector embeddings

ğŸ§  LLM-powered log explanation (local, private)

ğŸ›¡ï¸ Prompt-injection resistant by design

ğŸ§© Graceful degradation when LLM is unavailable

ğŸ“Š Streamlit UI for interactive analysis

ğŸ³ Dockerized backend for reproducibility

ğŸ§ª Synthetic + real log ingestion support

What Problem Does LogRAG Solve?

Traditional log search relies on:

Exact keyword matches

Manual inspection

Fragmented context across services

LogRAG enables:

Meaning-based log retrieval

Cross-service correlation

Natural-language explanations of failures


ğŸ—ï¸ System Architecture (High Level)
User (Streamlit UI)
        â”‚
        â–¼
Semantic Search (Sentence Transformers)
        â”‚
        â–¼
Top-K Relevant Logs
        â”‚
        â”œâ”€â”€â–¶ LLM Available? â”€â”€â–¶ Yes â”€â”€â–¶ Ollama (Local LLM)
        â”‚                      â”‚
        â”‚                      â–¼
        â”‚              Structured Explanation
        â”‚
        â””â”€â”€â–¶ No â”€â”€â–¶ Return RAG results + reason

Faster root-cause analysis

