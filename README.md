LogRAG is a local, secure Retrieval-Augmented Generation (RAG) system that helps engineers understand, diagnose, and resolve production issues by analyzing historical logs and generating structured explanations using a local Large Language Model (LLM).

The system is designed to remain partially functional even when the LLM backend is unavailable, making it resilient and production-aware.

ğŸš€ Key Features

ğŸ” Semantic Log Search using vector embeddings

ğŸ§  LLM-powered log explanation (local, private)

ğŸ›¡ï¸ Prompt-injection resistant by design

ğŸ§© Graceful degradation when LLM is unavailable

ğŸ“Š Streamlit UI for interactive analysis

ğŸ³ Dockerized backend for reproducibility

ğŸ§ª Synthetic + real log ingestion support

ğŸ§  What Problem Does LogRAG Solve?

Traditional log search relies on:

Exact keyword matches

Manual inspection

Fragmented context across services

LogRAG enables:

Meaning-based log retrieval

Cross-service correlation

Natural-language explanations of failures

ğŸ—ï¸ System Architecture (High Level)
User (Streamlit UI) --> Semantic Search (Sentence Transformers) --> Top-K Relevant Logs --> Structured Explanation / RAG results + reason (Ollama (Local LLM))

ğŸ§± Technology Stack & Responsibilities
ğŸ¨ Frontend

Streamlit

Interactive web UI

Accepts log/error input

Displays:

LLM explanation

Similar historical logs

Fallback indicators

âš™ï¸ Backend API

FastAPI

REST API for:

/search â†’ semantic log retrieval

/explain â†’ RAG + LLM explanation

/health â†’ system status

Handles:

Input sanitization

Graceful LLM fallback

Secure prompt construction

ğŸ§  Embedding Layer

SentenceTransformers

Model: all-MiniLM-L6-v2

Converts logs into dense semantic vectors

Enables similarity search beyond keywords

ğŸ“ Vector Similarity

Scikit-learn (Cosine Similarity)

Computes semantic similarity between:

User query

Stored log vectors

Retrieves Top-K relevant logs

ğŸ¤– LLM Engine

Ollama

Runs LLMs locally (e.g., mistral)

No cloud dependency

Ensures privacy & compliance

Used only after retrieval (RAG pattern)

ğŸ§ª Log Corpus

Synthetic + Real Logs

Synthetic logs simulate:

Application errors

Container failures

Network issues

Security events

Easily extensible to:

HDFS logs

OpenStack logs

Syslog

Web server logs

ğŸ³ Containerization

Docker & Docker Compose

Isolates:

API

UI

Enables reproducible deployment

Simplifies local testing

ğŸ”„ Retrieval-Augmented Generation (RAG) Flow

User submits an issue or log snippet

Query is embedded using SentenceTransformers

Similar logs are retrieved from vector store

Retrieved logs are passed as untrusted context

LLM generates:

Root cause

Impact

Suggested fix

If LLM fails:

System returns semantic results only

Explains degradation reason



